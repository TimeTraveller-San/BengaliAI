{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as albu\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import sklearn\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\n",
    "    42 is the answer to everything.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200840, 16389)\n"
     ]
    }
   ],
   "source": [
    "# Load Feather Data \n",
    "debug = False\n",
    "if debug:\n",
    "    df = '../data/train.csv'\n",
    "    df = pd.read_csv(df)\n",
    "    files = [f'../data/train_128_feather/train_{i}.feather' for i in range(4)]\n",
    "    data0 = pd.read_feather(files[0])\n",
    "    data_full = data0\n",
    "    del data0\n",
    "    gc.collect()\n",
    "    data_full = df.merge(data_full, on='image_id', how='inner')\n",
    "    del df\n",
    "    gc.collect()\n",
    "    print(data_full.shape)\n",
    "else:\n",
    "    df = '../data/train.csv'\n",
    "    df = pd.read_csv(df)\n",
    "    files = [f'../data/train_128_feather/train_{i}.feather' for i in range(4)]\n",
    "    data0 = pd.read_feather(files[0])\n",
    "    data1 = pd.read_feather(files[1])\n",
    "    data2 = pd.read_feather(files[2])\n",
    "    data3 = pd.read_feather(files[3])\n",
    "    data_full = pd.concat([data0,data1,data2,data3], ignore_index=True)\n",
    "    del data0, data1, data2, data3\n",
    "    gc.collect()\n",
    "    data_full = df.merge(data_full, on='image_id', how='inner')\n",
    "    del df\n",
    "    gc.collect()\n",
    "    print(data_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if debug:\n",
    "    LIMIT = 10000\n",
    "    data_full = data_full[:LIMIT]\n",
    "train_df , valid_df = train_test_split(data_full, test_size=0.20, random_state=42,shuffle=True) ## Split Labels\n",
    "del data_full \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            return tuple([self._to_tensor(image) for image in data])\n",
    "        else:\n",
    "            return self._to_tensor(data)\n",
    "\n",
    "    def _to_tensor(self, data):\n",
    "        if len(data.shape) == 3:\n",
    "            return torch.from_numpy(data.transpose(2, 0, 1).astype(np.float32))\n",
    "        else:\n",
    "            return torch.from_numpy(data[None, :, :].astype(np.float32))\n",
    "        \n",
    "class Normalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = np.average(mean)\n",
    "        self.std = np.average(std)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = np.asarray(image).astype(np.float32) / 255.\n",
    "        image = (image - self.mean) / self.std\n",
    "        return image    \n",
    "    \n",
    "train_aug = albu.Compose([ \n",
    "    albu.ShiftScaleRotate(p=0.8, border_mode=cv2.BORDER_CONSTANT, value =1),\n",
    "    albu.OneOf([\n",
    "        albu.ElasticTransform(p=0.1, alpha=1, sigma=10, alpha_affine=10, border_mode=cv2.BORDER_CONSTANT,value =1),\n",
    "        albu.GridDistortion(distort_limit =0.01 ,border_mode=cv2.BORDER_CONSTANT,value =1, p=0.1),\n",
    "        albu.OpticalDistortion(p=0.1, distort_limit= 0.01, shift_limit=0.1, border_mode=cv2.BORDER_CONSTANT,value =1)                  \n",
    "        ], p=0.3),\n",
    "    albu.OneOf([\n",
    "#         albu.GaussNoise(var_limit=0.5),\n",
    "        albu.Blur(),\n",
    "        albu.GaussianBlur(blur_limit=1)\n",
    "        ], p=0.4),    \n",
    "    albu.RandomGamma(p=0.8)\n",
    "])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BengaliAI(Dataset):\n",
    "    def __init__(self, data, details=False, transform=None, imgsize=(128, 128)):\n",
    "        self.images = data.iloc[:, 5:].values\n",
    "        self.grapheme_roots = data['grapheme_root'].values\n",
    "        self.vowel_diacritics = data['vowel_diacritic'].values\n",
    "        self.consonant_diacritics = data['consonant_diacritic'].values\n",
    "        self.imgsize = imgsize\n",
    "        self.transform = transform\n",
    "        if details:\n",
    "            self.mean, self.std = details\n",
    "        else:\n",
    "            self.mean, self.std = 0.5, 0.5\n",
    "        self.reqtransform = transforms.Compose([\n",
    "            Normalize(self.mean, self.std),\n",
    "            transforms.ToTensor()\n",
    "        ])    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].reshape(self.imgsize).astype(np.float)\n",
    "        grapheme_root = self.grapheme_roots[idx]\n",
    "        vowel_diacritic = self.vowel_diacritics[idx]\n",
    "        consonant_diacritic = self.consonant_diacritics[idx]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)['image']\n",
    "#         norm = Normalize(self.mean, self.std) #TODO: Determine these values using pretrainedmodels\n",
    "#         t = ToTensor()\n",
    "#         img = t._to_tensor(norm(img))\n",
    "        img = self.reqtransform(img)\n",
    "        label = (grapheme_root, vowel_diacritic, consonant_diacritic)\n",
    "        return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "\n",
    "if debug:    \n",
    "    dataset = BengaliAI(train_df, transform=train_aug)\n",
    "    i = 0\n",
    "    LIMIT = 10\n",
    "\n",
    "    for img, (l1, l2, l2) in dataset:\n",
    "        plt.imshow(img.numpy().reshape(128, 128))\n",
    "        plt.show()\n",
    "        i += 1\n",
    "        if i > LIMIT:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrainedmodels.__dict__[model_name](pretrained='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretrainedmodels\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "n_grapheme = 168\n",
    "n_vowel = 11\n",
    "n_consonant = 7\n",
    "num_classes = [n_grapheme, n_vowel, n_consonant]\n",
    "\n",
    "class ClassifierCNN(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=num_classes, pretrained='imagenet'):\n",
    "        super(ClassifierCNN, self).__init__()\n",
    "        \n",
    "        self.inconv = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        if model_name.split('-')[0] == 'efficientnet':\n",
    "            self.effnet = True\n",
    "#             self.model = EfficientNet.from_pretrained(model_name) \n",
    "            self.model = EfficientNet.from_name(model_name) \n",
    "            in_features = 1280 #TODO: Write a lazy linear to find this, for now, I do it by getting an error\n",
    "        else:\n",
    "            self.effnet = False\n",
    "            self.model = pretrainedmodels.__dict__[model_name](pretrained=pretrained)\n",
    "            in_features = self.model.last_linear.in_features\n",
    "            \n",
    "        self.head_grapheme_root = nn.Linear(in_features, num_classes[0])\n",
    "        self.head_vowel_diacritic = nn.Linear(in_features, num_classes[1])\n",
    "        self.head_consonant_diacritic = nn.Linear(in_features, num_classes[2])\n",
    "        \n",
    "    def freeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    def forward(self, x, logit=True):\n",
    "        x  = self.inconv(x)\n",
    "        if self.effnet:\n",
    "            features = self.model.extract_features(x)\n",
    "        else:\n",
    "            features = self.model.features(x)\n",
    "        features = F.adaptive_avg_pool2d(features, 1)\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        logit_grapheme_root = self.head_grapheme_root(features)\n",
    "        logit_vowel_diacritic = self.head_vowel_diacritic(features)\n",
    "        logit_consonant_diacritic = self.head_consonant_diacritic(features)\n",
    "        \n",
    "        if logit:\n",
    "            return logit_grapheme_root, logit_vowel_diacritic, logit_consonant_diacritic            \n",
    "        else:\n",
    "            grapheme_root = F.softmax(logit_grapheme_root, 1)\n",
    "            vowel_diacritic = F.softmax(logit_vowel_diacritic, 1)\n",
    "            consonant_diacritic = F.softmax(logit_consonant_diacritic, 1)\n",
    "            return grapheme_root, vowel_diacritic, consonant_diacritic            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std(model_name):\n",
    "    try:\n",
    "        mean = pretrainedmodels.__dict__['pretrained_settings'][model_name]['imagenet']['mean']\n",
    "        std = pretrainedmodels.__dict__['pretrained_settings'][model_name]['imagenet']['std']\n",
    "    except:\n",
    "        mean, std = 0.5, 0.5\n",
    "    return (mean, std)\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "# model_name = 'se_resnext101_32x4d'\n",
    "model_name = 'efficientnet-b0'\n",
    "model = ClassifierCNN(model_name).to(device)\n",
    "lr = 1e-3\n",
    "n_epochs = 30\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=lr\n",
    ")\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.3) \n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 1e-2, total_steps=None, epochs=n_epochs, steps_per_epoch=3139, pct_start=0.0,\n",
    "#                                    anneal_strategy='cos', cycle_momentum=True,base_momentum=0.85, max_momentum=0.95,  div_factor=100.0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset = BengaliAI(train_df, transform=train_aug, details=mean_std(model_name))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n",
    "\n",
    "val_dataset = BengaliAI(valid_df, details=mean_std(model_name))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 168])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((batch_size,1, 64, 64))\n",
    "with torch.no_grad():\n",
    "    output1, output2, output3 =model(x.cuda())\n",
    "print(output1.shape)\n",
    "print(output2.shape)\n",
    "print(output3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_recall_multi(preds, labels):\n",
    "    pred_graphemes, pred_vowels, pred_consonants = preds\n",
    "    true_graphemes, true_vowels, true_consonants = labels\n",
    "    n_grapheme = 168\n",
    "    n_vowel = 11\n",
    "    n_consonant = 7\n",
    "    pred_label_graphemes = torch.argmax(pred_graphemes, dim=1).cpu().numpy()\n",
    "    true_label_graphemes = true_graphemes.cpu().numpy()\n",
    "    pred_label_vowels = torch.argmax(pred_vowels, dim=1).cpu().numpy()\n",
    "    true_label_vowels = true_vowels.cpu().numpy()\n",
    "    pred_label_consonants = torch.argmax(pred_consonants, dim=1).cpu().numpy()\n",
    "    true_label_consonants = true_consonants.cpu().numpy()    \n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(pred_label_graphemes, true_label_graphemes, average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(pred_label_vowels, true_label_vowels, average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(pred_label_consonants, true_label_consonants, average='macro')\n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_score = np.average(scores, weights=[2, 1, 1])\n",
    "    return final_score, recall_grapheme, recall_vowel, recall_consonant\n",
    "\n",
    "\n",
    "def calc_macro_recall(solution, submission):\n",
    "    # solution df, submission df\n",
    "    scores = []\n",
    "    for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n",
    "        y_true_subset = solution[solution[component] == component]['target'].values\n",
    "        y_pred_subset = submission[submission[component] == component]['target'].values\n",
    "        scores.append(sklearn.metrics.recall_score(\n",
    "            y_true_subset, y_pred_subset, average='macro'))\n",
    "    final_score = np.average(scores, weights=[2, 1, 1])\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1d4c258bda463a82dee388ec34cfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f311de058ff74f3db53dec7f7186588a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5021), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timetraveller/.local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# model.freeze()\n",
    "ws = [0.5, 0.25, 0.25]\n",
    "history = pd.DataFrame()\n",
    "\n",
    "if n_epochs:    \n",
    "    for epoch in tqdm2(range(n_epochs)):\n",
    "        \n",
    "        running_loss = 0\n",
    "        running_loss0 = 0\n",
    "        running_loss1 = 0\n",
    "        running_loss2 = 0\n",
    "        \n",
    "        running_acc0 = 0.0\n",
    "        running_acc1 = 0.0\n",
    "        running_acc2 = 0.0\n",
    "        \n",
    "        running_recall = 0.0\n",
    "        running_recall0 = 0.0\n",
    "        running_recall1 = 0.0\n",
    "        running_recall2 = 0.0\n",
    "        \n",
    "        recall = 0\n",
    "        \n",
    "        bar = tqdm2(train_loader)\n",
    "        for i, (img, label) in enumerate(bar):\n",
    "            img = img.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(img)    \n",
    "            label[0] = label[0].to(device)\n",
    "            label[1] = label[1].to(device)\n",
    "            label[2] = label[2].to(device)\n",
    "            loss0 = criterion(out[0], label[0])\n",
    "            loss1 = criterion(out[1], label[1])\n",
    "            loss2 = criterion(out[2], label[2])\n",
    "            loss = ws[0]*loss0 + ws[1]*loss1 + ws[2]*loss2\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            bar.set_description(f\"Recall: {recall:.3f}\")\n",
    "            with torch.no_grad():\n",
    "            \n",
    "                running_loss += loss.item()/len(train_loader)\n",
    "                running_loss0 += loss0.item()/len(train_loader)\n",
    "                running_loss1 += loss1.item()/len(train_loader)\n",
    "                running_loss2 += loss2.item()/len(train_loader)\n",
    "                \n",
    "                recall, recall_grapheme, recall_vowel, recall_consonant = macro_recall_multi(out, label)\n",
    "                \n",
    "                running_recall += recall/len(train_loader)\n",
    "                running_recall0 += recall_grapheme/len(train_loader)\n",
    "                running_recall1 += recall_vowel/len(train_loader)\n",
    "                running_recall2 += recall_consonant/len(train_loader)\n",
    "                \n",
    "                running_acc0 += (out[0].argmax(1)==label[0]).float().mean()/len(train_loader)\n",
    "                running_acc1 += (out[1].argmax(1)==label[1]).float().mean()/len(train_loader)\n",
    "                running_acc2 += (out[2].argmax(1)==label[2]).float().mean()/len(train_loader)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch: [{epoch+1}/{n_epochs}] Training...\")\n",
    "        print(f\"Recall: {running_recall:.3f} | [{running_recall0:.3f} | {running_recall1:.3f} | {running_recall2:.3f}]\")        \n",
    "        print(f\"Acc:  [{100*running_acc0:.3f}% | {100*running_acc1:.3f}% | {100*running_acc2:.3f}%]\")                \n",
    "        print(f\"Loss: {running_loss:.3f} | [{running_loss0:.3f} | {running_loss1:.3f} | {running_loss2:.3f}]\")\n",
    "        \n",
    "        history.loc[epoch, 'train_loss'] = running_loss\n",
    "        history.loc[epoch, 'train_recall'] = running_recall\n",
    "        history.loc[epoch, 'train_acc_grapheme'] = running_acc0.cpu().numpy()\n",
    "        history.loc[epoch, 'train_acc_vowel'] = running_acc1.cpu().numpy()\n",
    "        history.loc[epoch, 'train_acc_consonant'] = running_acc2.cpu().numpy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_loss = 0\n",
    "            running_loss0 = 0\n",
    "            running_loss1 = 0\n",
    "            running_loss2 = 0\n",
    "\n",
    "            running_acc0 = 0.0\n",
    "            running_acc1 = 0.0\n",
    "            running_acc2 = 0.0\n",
    "\n",
    "            running_recall = 0.0\n",
    "            running_recall0 = 0.0\n",
    "            running_recall1 = 0.0\n",
    "            running_recall2 = 0.0\n",
    "\n",
    "        \n",
    "            for i, (img, label) in enumerate(val_loader):\n",
    "                img = img.to(device)\n",
    "                out = model(img)    \n",
    "                label[0] = label[0].to(device)\n",
    "                label[1] = label[1].to(device)\n",
    "                label[2] = label[2].to(device)\n",
    "                recall, recall_grapheme, recall_vowel, recall_consonant = macro_recall_multi(out, label)\n",
    "                running_recall += recall/len(val_loader)\n",
    "                running_recall0 += recall_grapheme/len(val_loader)\n",
    "                running_recall1 += recall_vowel/len(val_loader)\n",
    "                running_recall2 += recall_consonant/len(val_loader)\n",
    "                running_acc0 += (out[0].argmax(1)==label[0]).float().mean()/len(val_loader)\n",
    "                running_acc1 += (out[1].argmax(1)==label[1]).float().mean()/len(val_loader)\n",
    "                running_acc2 += (out[2].argmax(1)==label[2]).float().mean()/len(val_loader)\n",
    "            print(f\"Epoch: [{epoch+1}/{n_epochs}] Validating...\")\n",
    "            print(f\"Recall: {running_recall:.3f} | [{running_recall0:.3f} | {running_recall1:.3f} | {running_recall2:.3f}]\")        \n",
    "            print(f\"Acc:  [{100*running_acc0:.3f}% | {100*running_acc1:.3f}% | {100*running_acc2:.3f}%]\")     \n",
    "            \n",
    "            history.loc[epoch, 'val_recall'] = running_recall\n",
    "            history.loc[epoch, 'val_acc_grapheme'] = running_acc0.cpu().numpy()\n",
    "            history.loc[epoch, 'val_acc_vowel'] = running_acc1.cpu().numpy()\n",
    "            history.loc[epoch, 'val_acc_consonant'] = running_acc2.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTTTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not-pretrained-exp1.csv  pretrained-exp1.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np = pd.read_csv(\"../logs/not-pretrained-exp1.csv\").drop(['Unnamed: 0'], axis=1)\n",
    "p = pd.read_csv(\"../logs/pretrained-exp1.csv\").drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_acc_grapheme</th>\n",
       "      <th>train_acc_vowel</th>\n",
       "      <th>train_acc_consonant</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_acc_grapheme</th>\n",
       "      <th>val_acc_vowel</th>\n",
       "      <th>val_acc_consonant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.505188</td>\n",
       "      <td>0.476433</td>\n",
       "      <td>0.404644</td>\n",
       "      <td>0.754619</td>\n",
       "      <td>0.801698</td>\n",
       "      <td>0.743615</td>\n",
       "      <td>0.747239</td>\n",
       "      <td>0.912694</td>\n",
       "      <td>0.917646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.515448</td>\n",
       "      <td>0.767685</td>\n",
       "      <td>0.776719</td>\n",
       "      <td>0.915721</td>\n",
       "      <td>0.921272</td>\n",
       "      <td>0.811384</td>\n",
       "      <td>0.827480</td>\n",
       "      <td>0.936008</td>\n",
       "      <td>0.935037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.376225</td>\n",
       "      <td>0.825291</td>\n",
       "      <td>0.837834</td>\n",
       "      <td>0.939543</td>\n",
       "      <td>0.939769</td>\n",
       "      <td>0.851313</td>\n",
       "      <td>0.862586</td>\n",
       "      <td>0.952254</td>\n",
       "      <td>0.948646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.311444</td>\n",
       "      <td>0.852571</td>\n",
       "      <td>0.865452</td>\n",
       "      <td>0.950254</td>\n",
       "      <td>0.949413</td>\n",
       "      <td>0.862851</td>\n",
       "      <td>0.875599</td>\n",
       "      <td>0.953523</td>\n",
       "      <td>0.956633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.272624</td>\n",
       "      <td>0.869646</td>\n",
       "      <td>0.882967</td>\n",
       "      <td>0.956389</td>\n",
       "      <td>0.955902</td>\n",
       "      <td>0.877221</td>\n",
       "      <td>0.890950</td>\n",
       "      <td>0.961435</td>\n",
       "      <td>0.961012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.245733</td>\n",
       "      <td>0.880950</td>\n",
       "      <td>0.893137</td>\n",
       "      <td>0.960847</td>\n",
       "      <td>0.959275</td>\n",
       "      <td>0.881372</td>\n",
       "      <td>0.894731</td>\n",
       "      <td>0.962082</td>\n",
       "      <td>0.960265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.223936</td>\n",
       "      <td>0.890622</td>\n",
       "      <td>0.903265</td>\n",
       "      <td>0.963866</td>\n",
       "      <td>0.961967</td>\n",
       "      <td>0.890289</td>\n",
       "      <td>0.904260</td>\n",
       "      <td>0.964446</td>\n",
       "      <td>0.962630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.210154</td>\n",
       "      <td>0.896671</td>\n",
       "      <td>0.908588</td>\n",
       "      <td>0.966583</td>\n",
       "      <td>0.964345</td>\n",
       "      <td>0.897883</td>\n",
       "      <td>0.911923</td>\n",
       "      <td>0.969123</td>\n",
       "      <td>0.964495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.195881</td>\n",
       "      <td>0.902503</td>\n",
       "      <td>0.913735</td>\n",
       "      <td>0.968219</td>\n",
       "      <td>0.967006</td>\n",
       "      <td>0.902158</td>\n",
       "      <td>0.913715</td>\n",
       "      <td>0.970642</td>\n",
       "      <td>0.966636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.185157</td>\n",
       "      <td>0.907468</td>\n",
       "      <td>0.918703</td>\n",
       "      <td>0.970246</td>\n",
       "      <td>0.968157</td>\n",
       "      <td>0.901726</td>\n",
       "      <td>0.913889</td>\n",
       "      <td>0.971264</td>\n",
       "      <td>0.968849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.177739</td>\n",
       "      <td>0.911018</td>\n",
       "      <td>0.921994</td>\n",
       "      <td>0.971414</td>\n",
       "      <td>0.969718</td>\n",
       "      <td>0.901185</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.969969</td>\n",
       "      <td>0.967207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.169441</td>\n",
       "      <td>0.914322</td>\n",
       "      <td>0.925394</td>\n",
       "      <td>0.972416</td>\n",
       "      <td>0.970725</td>\n",
       "      <td>0.908016</td>\n",
       "      <td>0.917845</td>\n",
       "      <td>0.972781</td>\n",
       "      <td>0.971686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.162710</td>\n",
       "      <td>0.917492</td>\n",
       "      <td>0.927951</td>\n",
       "      <td>0.973350</td>\n",
       "      <td>0.972490</td>\n",
       "      <td>0.908988</td>\n",
       "      <td>0.917794</td>\n",
       "      <td>0.971164</td>\n",
       "      <td>0.972657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  train_recall  train_acc_grapheme  train_acc_vowel  \\\n",
       "0     1.505188      0.476433            0.404644         0.754619   \n",
       "1     0.515448      0.767685            0.776719         0.915721   \n",
       "2     0.376225      0.825291            0.837834         0.939543   \n",
       "3     0.311444      0.852571            0.865452         0.950254   \n",
       "4     0.272624      0.869646            0.882967         0.956389   \n",
       "5     0.245733      0.880950            0.893137         0.960847   \n",
       "6     0.223936      0.890622            0.903265         0.963866   \n",
       "7     0.210154      0.896671            0.908588         0.966583   \n",
       "8     0.195881      0.902503            0.913735         0.968219   \n",
       "9     0.185157      0.907468            0.918703         0.970246   \n",
       "10    0.177739      0.911018            0.921994         0.971414   \n",
       "11    0.169441      0.914322            0.925394         0.972416   \n",
       "12    0.162710      0.917492            0.927951         0.973350   \n",
       "\n",
       "    train_acc_consonant  val_recall  val_acc_grapheme  val_acc_vowel  \\\n",
       "0              0.801698    0.743615          0.747239       0.912694   \n",
       "1              0.921272    0.811384          0.827480       0.936008   \n",
       "2              0.939769    0.851313          0.862586       0.952254   \n",
       "3              0.949413    0.862851          0.875599       0.953523   \n",
       "4              0.955902    0.877221          0.890950       0.961435   \n",
       "5              0.959275    0.881372          0.894731       0.962082   \n",
       "6              0.961967    0.890289          0.904260       0.964446   \n",
       "7              0.964345    0.897883          0.911923       0.969123   \n",
       "8              0.967006    0.902158          0.913715       0.970642   \n",
       "9              0.968157    0.901726          0.913889       0.971264   \n",
       "10             0.969718    0.901185          0.913043       0.969969   \n",
       "11             0.970725    0.908016          0.917845       0.972781   \n",
       "12             0.972490    0.908988          0.917794       0.971164   \n",
       "\n",
       "    val_acc_consonant  \n",
       "0            0.917646  \n",
       "1            0.935037  \n",
       "2            0.948646  \n",
       "3            0.956633  \n",
       "4            0.961012  \n",
       "5            0.960265  \n",
       "6            0.962630  \n",
       "7            0.964495  \n",
       "8            0.966636  \n",
       "9            0.968849  \n",
       "10           0.967207  \n",
       "11           0.971686  \n",
       "12           0.972657  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_acc_grapheme</th>\n",
       "      <th>train_acc_vowel</th>\n",
       "      <th>train_acc_consonant</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_acc_grapheme</th>\n",
       "      <th>val_acc_vowel</th>\n",
       "      <th>val_acc_consonant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.625307</td>\n",
       "      <td>0.743177</td>\n",
       "      <td>0.737785</td>\n",
       "      <td>0.900265</td>\n",
       "      <td>0.909795</td>\n",
       "      <td>0.836936</td>\n",
       "      <td>0.850693</td>\n",
       "      <td>0.942973</td>\n",
       "      <td>0.943844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.305778</td>\n",
       "      <td>0.855398</td>\n",
       "      <td>0.866846</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.951884</td>\n",
       "      <td>0.869518</td>\n",
       "      <td>0.883237</td>\n",
       "      <td>0.956259</td>\n",
       "      <td>0.954767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.246591</td>\n",
       "      <td>0.879242</td>\n",
       "      <td>0.891446</td>\n",
       "      <td>0.960294</td>\n",
       "      <td>0.959119</td>\n",
       "      <td>0.878773</td>\n",
       "      <td>0.891647</td>\n",
       "      <td>0.963277</td>\n",
       "      <td>0.957753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.215192</td>\n",
       "      <td>0.893393</td>\n",
       "      <td>0.904441</td>\n",
       "      <td>0.965787</td>\n",
       "      <td>0.964318</td>\n",
       "      <td>0.888841</td>\n",
       "      <td>0.898015</td>\n",
       "      <td>0.966959</td>\n",
       "      <td>0.961286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.194135</td>\n",
       "      <td>0.903426</td>\n",
       "      <td>0.914053</td>\n",
       "      <td>0.968965</td>\n",
       "      <td>0.967372</td>\n",
       "      <td>0.896166</td>\n",
       "      <td>0.909212</td>\n",
       "      <td>0.966859</td>\n",
       "      <td>0.963774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.179257</td>\n",
       "      <td>0.909576</td>\n",
       "      <td>0.920047</td>\n",
       "      <td>0.971166</td>\n",
       "      <td>0.969550</td>\n",
       "      <td>0.900414</td>\n",
       "      <td>0.913192</td>\n",
       "      <td>0.967855</td>\n",
       "      <td>0.966585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.165977</td>\n",
       "      <td>0.915260</td>\n",
       "      <td>0.925356</td>\n",
       "      <td>0.973182</td>\n",
       "      <td>0.971465</td>\n",
       "      <td>0.893938</td>\n",
       "      <td>0.905480</td>\n",
       "      <td>0.967831</td>\n",
       "      <td>0.965366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.156747</td>\n",
       "      <td>0.919602</td>\n",
       "      <td>0.929874</td>\n",
       "      <td>0.974443</td>\n",
       "      <td>0.972573</td>\n",
       "      <td>0.900766</td>\n",
       "      <td>0.912744</td>\n",
       "      <td>0.969546</td>\n",
       "      <td>0.965914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.148719</td>\n",
       "      <td>0.923348</td>\n",
       "      <td>0.932575</td>\n",
       "      <td>0.975806</td>\n",
       "      <td>0.974275</td>\n",
       "      <td>0.907084</td>\n",
       "      <td>0.918765</td>\n",
       "      <td>0.971513</td>\n",
       "      <td>0.968327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.142007</td>\n",
       "      <td>0.926574</td>\n",
       "      <td>0.935849</td>\n",
       "      <td>0.976433</td>\n",
       "      <td>0.974911</td>\n",
       "      <td>0.908876</td>\n",
       "      <td>0.918491</td>\n",
       "      <td>0.974325</td>\n",
       "      <td>0.971089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.134877</td>\n",
       "      <td>0.929197</td>\n",
       "      <td>0.938705</td>\n",
       "      <td>0.977389</td>\n",
       "      <td>0.975737</td>\n",
       "      <td>0.902041</td>\n",
       "      <td>0.912297</td>\n",
       "      <td>0.974697</td>\n",
       "      <td>0.968526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.130699</td>\n",
       "      <td>0.931716</td>\n",
       "      <td>0.940983</td>\n",
       "      <td>0.978273</td>\n",
       "      <td>0.976905</td>\n",
       "      <td>0.902803</td>\n",
       "      <td>0.915083</td>\n",
       "      <td>0.971812</td>\n",
       "      <td>0.967655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.126918</td>\n",
       "      <td>0.933043</td>\n",
       "      <td>0.942165</td>\n",
       "      <td>0.978534</td>\n",
       "      <td>0.976950</td>\n",
       "      <td>0.905201</td>\n",
       "      <td>0.913142</td>\n",
       "      <td>0.973678</td>\n",
       "      <td>0.971960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  train_recall  train_acc_grapheme  train_acc_vowel  \\\n",
       "0     0.625307      0.743177            0.737785         0.900265   \n",
       "1     0.305778      0.855398            0.866846         0.951049   \n",
       "2     0.246591      0.879242            0.891446         0.960294   \n",
       "3     0.215192      0.893393            0.904441         0.965787   \n",
       "4     0.194135      0.903426            0.914053         0.968965   \n",
       "5     0.179257      0.909576            0.920047         0.971166   \n",
       "6     0.165977      0.915260            0.925356         0.973182   \n",
       "7     0.156747      0.919602            0.929874         0.974443   \n",
       "8     0.148719      0.923348            0.932575         0.975806   \n",
       "9     0.142007      0.926574            0.935849         0.976433   \n",
       "10    0.134877      0.929197            0.938705         0.977389   \n",
       "11    0.130699      0.931716            0.940983         0.978273   \n",
       "12    0.126918      0.933043            0.942165         0.978534   \n",
       "\n",
       "    train_acc_consonant  val_recall  val_acc_grapheme  val_acc_vowel  \\\n",
       "0              0.909795    0.836936          0.850693       0.942973   \n",
       "1              0.951884    0.869518          0.883237       0.956259   \n",
       "2              0.959119    0.878773          0.891647       0.963277   \n",
       "3              0.964318    0.888841          0.898015       0.966959   \n",
       "4              0.967372    0.896166          0.909212       0.966859   \n",
       "5              0.969550    0.900414          0.913192       0.967855   \n",
       "6              0.971465    0.893938          0.905480       0.967831   \n",
       "7              0.972573    0.900766          0.912744       0.969546   \n",
       "8              0.974275    0.907084          0.918765       0.971513   \n",
       "9              0.974911    0.908876          0.918491       0.974325   \n",
       "10             0.975737    0.902041          0.912297       0.974697   \n",
       "11             0.976905    0.902803          0.915083       0.971812   \n",
       "12             0.976950    0.905201          0.913142       0.973678   \n",
       "\n",
       "    val_acc_consonant  \n",
       "0            0.943844  \n",
       "1            0.954767  \n",
       "2            0.957753  \n",
       "3            0.961286  \n",
       "4            0.963774  \n",
       "5            0.966585  \n",
       "6            0.965366  \n",
       "7            0.965914  \n",
       "8            0.968327  \n",
       "9            0.971089  \n",
       "10           0.968526  \n",
       "11           0.967655  \n",
       "12           0.971960  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
